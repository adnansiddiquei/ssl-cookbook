# Contrastive Learning
The goal of contrastive learning is to learn meaningful (often lower dimensional) representations of 
objects by training a model to maximise the similarity between embedded views of the same object,
and minimise similarity between embedded view of different objects.

In modern contrastive learning, pairs of samples are generally artificially generated by semantically
augmenting (that is, augmenting an object whilst maintaining it's meaning) existing data.

For example, you may have a grey-scale image $X \in \mathbb{R}^{m \times n}$ which can be 
augmented through rotation, flipping, cropping, or adding noise. These augmentations create 
different views of the same underlying image.